# PytorchResearchCodesReading
An organized list of open source code for some research papers I have read.


## SSKD: Knowledge Distillation Meets Self-Supervision

The implementation of paper [Knowledge Distillation Meets Self-Supervision](https://github.com/xuguodong03/SSKD) (ECCV 2020).
<!-- ![架构](https://github.com/xuguodong03/SSKD/raw/master/frm.png) -->
<center><img src="https://github.com/xuguodong03/SSKD/raw/master/frm.png"> </center>


## FedX: Unsupervised Federated Learning with Cross Knowledge Distillation

The PyTorch code of paper [FedX: Unsupervised Federated Learning with Cross Knowledge Distillation](https://arxiv.org/abs/2207.09158) (ECCV 2022).



## CRD: Contrastive Representation Distillation

The implementation of the following ICLR 2020 paper [Contrastive Representation Distillation (CRD)](https://github.com/HobbitLong/RepDistiller). [Paper](http://arxiv.org/abs/1910.10699), [Project Page](http://hobbitlong.github.io/CRD/).
<center><img src="https://camo.githubusercontent.com/1dc5b188095cfa936653171b3cd23793d872c1c25438a034ade65c4db382f22d/687474703a2f2f686f626269746c6f6e672e6769746875622e696f2f4352442f4352445f66696c65732f7465617365722e6a7067"></center>


## MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks

The official pytorch implementation of our paper [MEAL V2: Boosting Vanilla ResNet-50 to 80%+ Top-1 Accuracy on ImageNet without Tricks](https://github.com/szq0214/MEAL-V2)
![架构图](https://user-images.githubusercontent.com/3794909/92182326-6f78c400-ee19-11ea-80e4-2d6e4d73ce82.png)


